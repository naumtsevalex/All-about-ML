{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirius3085/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "MODEL_DIR = \"saved_e5_model\"\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModel.from_pretrained(MODEL_DIR)\n",
    "model.eval()  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(texts, prefix=\"query\", batch_size=32):\n",
    "    \"\"\"\n",
    "    –ö–æ–¥–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è [CLS] —Ç–æ–∫–µ–Ω).\n",
    "    prefix: \"query\" –∏–ª–∏ \"passage\" (–¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —à–∞–±–ª–æ–Ω–∞).\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            [f\"{prefix}: {text}\" for text in batch],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "            cls_emb = output.last_hidden_state[:, 0]  # [CLS] —Ç–æ–∫–µ–Ω\n",
    "            embeddings.append(cls_emb.cpu())\n",
    "\n",
    "    return torch.cat(embeddings, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–ø—Ä–æ—Å–æ–≤: (2, 384)\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞\n",
    "queries = [\"wireless bluetooth headphones\", \"usb-c charging cable\"]\n",
    "query_embs = encode_texts(queries, prefix=\"query\")\n",
    "\n",
    "print(\"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–ø—Ä–æ—Å–æ–≤:\", query_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ---\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "PRODUCTS_PATH = \"product_titles.csv\"\n",
    "EMBEDS_PATH = \"product_embeddings.npy\"\n",
    "# EMBEDS_PATH = \"small_product_embeddings.npy\"\n",
    "MODEL_PATH = \"saved_e5_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìÑ –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2027874/2027874 [03:59<00:00, 8483.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõçÔ∏è –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: 1423918\n"
     ]
    }
   ],
   "source": [
    "# --- 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ ---\n",
    "if os.path.exists(PRODUCTS_PATH):\n",
    "    print(\"üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...\")\n",
    "    product_titles = pd.read_csv(PRODUCTS_PATH).squeeze().tolist()\n",
    "    print(f\"‚úÖ –ó–∞–≥—Ä—É–∑–∏–ª–∏ {len(product_titles)} —Ç–æ–≤–∞—Ä–æ–≤\")\n",
    "else:\n",
    "    print(\"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å Hugging Face...\")\n",
    "    dataset = load_dataset(\"tasksource/esci\", split=\"train\")\n",
    "    df = pd.DataFrame([x for x in tqdm(dataset, desc=\"üìÑ –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame\")])\n",
    "\n",
    "    product_titles = df['product_title'].dropna().unique().tolist()\n",
    "    print(f\"üõçÔ∏è –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {len(product_titles)}\")\n",
    "    pd.Series(product_titles).to_csv(PRODUCTS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(texts, prefix=\"passage\", batch_size=64):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"üöÄ –ö–æ–¥–∏—Ä—É–µ–º —Ç–æ–≤–∞—Ä—ã\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer([f\"{prefix}: {t}\" for t in batch], padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=inputs['input_ids'].cuda(),\n",
    "                attention_mask=inputs['attention_mask'].cuda()\n",
    "            )\n",
    "        emb = outputs.last_hidden_state[:, 0].cpu().numpy()\n",
    "        all_embeddings.append(emb)\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üöÄ –ö–æ–¥–∏—Ä—É–µ–º —Ç–æ–≤–∞—Ä—ã:   0%|          | 0/11125 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "üöÄ –ö–æ–¥–∏—Ä—É–µ–º —Ç–æ–≤–∞—Ä—ã: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11125/11125 [09:13<00:00, 20.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "print(\"üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å...\")\n",
    "model = AutoModel.from_pretrained(MODEL_PATH).eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "product_embs = encode_texts(product_titles, batch_size=128)\n",
    "np.save(EMBEDS_PATH, product_embs)\n",
    "print(\"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üöÄ –ö–æ–¥–∏—Ä—É–µ–º —Ç–æ–≤–∞—Ä—ã: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 60.86it/s]\n",
      "üöÄ –ö–æ–¥–∏—Ä—É–µ–º —Ç–æ–≤–∞—Ä—ã: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Similarity score: 0.9837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_product_title = \"Wireless Headset\"\n",
    "_query = \"wireless headphones\"\n",
    "\n",
    "# –ö–æ–¥–∏—Ä—É–µ–º —Å –Ω—É–∂–Ω—ã–º–∏ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏\n",
    "t = encode_texts([_product_title], prefix=\"passage\")  # ‚Üí (1, dim)\n",
    "q = encode_texts([_query], prefix=\"query\")            # ‚Üí (1, dim)\n",
    "\n",
    "# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "t = t / np.linalg.norm(t, axis=1, keepdims=True)\n",
    "q = q / np.linalg.norm(q, axis=1, keepdims=True)\n",
    "\n",
    "# –°–∫–∞–ª–∞—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ ‚Üí –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å\n",
    "score = np.dot(q, t.T)[0][0]\n",
    "\n",
    "print(f\"üìä Similarity score: {score:.4f}\")\n",
    "# +- —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –ø—Ä–æ–¥–æ–º!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìÑ –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2027874/2027874 [04:00<00:00, 8425.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–æ–≤–∞—Ä–∞–º...\n",
      "‚úÖ –°–æ—Ö—Ä–∞–Ω–∏–ª–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è 1423918 —Ç–æ–≤–∞—Ä–æ–≤\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–∫–∞–∑–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞\n",
    "# ‚Äî‚Äî‚Äî –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç, –µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω ‚Äî‚Äî‚Äî\n",
    "if not os.path.exists(PRODUCTS_PATH) or not os.path.exists(\"product_stats.csv\"):\n",
    "    print(\"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å Hugging Face...\")\n",
    "    dataset = load_dataset(\"tasksource/esci\", split=\"train\")\n",
    "    df = pd.DataFrame([x for x in tqdm(dataset, desc=\"üìÑ –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame\")])\n",
    "    \n",
    "    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã\n",
    "    product_titles = df['product_title'].dropna().unique().tolist()\n",
    "    pd.Series(product_titles).to_csv(PRODUCTS_PATH, index=False)\n",
    "\n",
    "    # ‚Äî‚Äî‚Äî –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∫–∞–∑–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞ ‚Äî‚Äî‚Äî\n",
    "    print(\"üìä –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–æ–≤–∞—Ä–∞–º...\")\n",
    "    stats = df['product_title'].value_counts().reset_index()\n",
    "    stats.columns = ['product_title', 'views']\n",
    "    stats.to_csv(\"product_stats.csv\", index=False)\n",
    "    print(f\"‚úÖ –°–æ—Ö—Ä–∞–Ω–∏–ª–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è {len(stats)} —Ç–æ–≤–∞—Ä–æ–≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stats = pd.read_csv(\"product_stats.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_title</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pilot</th>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Episode 1</th>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Currently Unsupervised Novelty Graphic Sarcastic Funny T Shirt XL Black</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Â§â„Å™ÂÆ∂</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acer („Ç®„Ç§„Çµ„Éº) Aspire 5 „Çπ„É™„É† „Éé„Éº„Éà„Éë„ÇΩ„Ç≥„É≥ 15.6„Ç§„É≥„ÉÅ „Éï„É´HD IPS „Éá„Ç£„Çπ„Éó„É¨„Ç§ AMD(„Ç¢„Éâ„Éê„É≥„Çπ„Éà„Éª„Éû„Ç§„ÇØ„É≠„Éª„Éá„Éê„Ç§„Çª„Ç∫) Ryzen(„É©„Ç§„Çº„É≥) 3 3200U (Êó•Êú¨Ë™ûÈÖçÂàó„Åß„ÅØ„Å™„ÅÑÂ†¥Âêà„Åå„ÅÇ„Çä„Åæ„Åô)</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    views\n",
       "product_title                                            \n",
       "Pilot                                                 161\n",
       "Episode 1                                             157\n",
       "Currently Unsupervised Novelty Graphic Sarcasti...     92\n",
       "Â§â„Å™ÂÆ∂                                                    88\n",
       "Acer („Ç®„Ç§„Çµ„Éº) Aspire 5 „Çπ„É™„É† „Éé„Éº„Éà„Éë„ÇΩ„Ç≥„É≥ 15.6„Ç§„É≥„ÉÅ „Éï„É´HD I...     85"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
