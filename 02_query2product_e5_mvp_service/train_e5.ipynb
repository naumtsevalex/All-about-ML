{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ E5 –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirius3085/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import TripletMarginLoss\n",
    "from torch.nn.functional import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "# ‚Äî‚Äî‚Äî 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ‚Äî‚Äî‚Äî\n",
    "# def dummy_tokenize(text: str):\n",
    "#     return text.lower()\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=64, num_negatives=10):\n",
    "        self.samples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_negatives = num_negatives\n",
    "        self.all_products = df['product_title'].unique()\n",
    "        self._build_triplets(df)\n",
    "\n",
    "    def _build_triplets(self, df):\n",
    "        n = len(df)\n",
    "        for i in range(self.num_negatives):\n",
    "            negs = random.choices(self.all_products, k=n)\n",
    "            for idx, row in enumerate(df.itertuples(index=False)):\n",
    "                query = row.query\n",
    "                pos = row.product_title\n",
    "                neg = negs[idx]\n",
    "                self.samples.append((query, pos, neg))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q, pos, neg = self.samples[idx]\n",
    "\n",
    "        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å—Ä–∞–∑—É –¥–ª—è –º–æ–¥–µ–ª–∏\n",
    "        anchor_enc = self.tokenizer(\n",
    "            f\"query: {q}\", padding='max_length', truncation=True,\n",
    "            max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "        pos_enc = self.tokenizer(\n",
    "            f\"passage: {pos}\", padding='max_length', truncation=True,\n",
    "            max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "        neg_enc = self.tokenizer(\n",
    "            f\"passage: {neg}\", padding='max_length', truncation=True,\n",
    "            max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ input_ids –∏ attention_mask, –∫–∞–∫ –Ω—É–∂–Ω–æ –¥–ª—è forward\n",
    "        return (\n",
    "            (anchor_enc['input_ids'].squeeze(0), anchor_enc['attention_mask'].squeeze(0)),\n",
    "            (pos_enc['input_ids'].squeeze(0), pos_enc['attention_mask'].squeeze(0)),\n",
    "            (neg_enc['input_ids'].squeeze(0), neg_enc['attention_mask'].squeeze(0)),\n",
    "\n",
    "            f\"query: {q}\", # –¥–ª—è –¥–µ–±–∞–≥–∞\n",
    "            f\"passage: {pos}\",\n",
    "            f\"passage: {neg}\"\n",
    "\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import pytorch_lightning as pl\n",
    "from datetime import datetime\n",
    "\n",
    "# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ TripletDataset –∏ E5Model —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã —Ä–∞–Ω–µ–µ\n",
    "\n",
    "# ‚Äî‚Äî‚Äî 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ‚Äî‚Äî‚Äî\n",
    "def prepare_data(model_name='intfloat/e5-small', batch_size=16, sample_rate=1.0):\n",
    "    print(\"üîΩ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç tasksource/esci...\")\n",
    "    dataset = load_dataset(\"tasksource/esci\", split=\"train\")\n",
    "    dataset_len = len(dataset)\n",
    "\n",
    "    # —Å–µ–º–ø–ª–∏–º —á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–µ–µ –æ—Ç–¥–µ–±–∞–∂–∏—Ç—å!\n",
    "    dataset_current_len = int(dataset_len * sample_rate)\n",
    "    dataset = dataset.shuffle(seed=42).select(range(dataset_current_len))\n",
    "\n",
    "    print(\"üì¶ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ pandas DataFrame...\")\n",
    "    df = pd.DataFrame([x for x in tqdm(dataset, desc=\"‚Üí –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫\")])\n",
    "\n",
    "    print(\"üßπ –§–∏–ª—å—Ç—Ä—É–µ–º –∫–ª–∞—Å—Å—ã: Exact / Substitute / Irrelevant...\")\n",
    "    df = df[df['esci_label'].isin(['Exact', 'Substitute', 'Irrelevant'])]\n",
    "\n",
    "    print(\"üîç –£–¥–∞–ª—è–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å < 2 –ø—Ä–∏–º–µ—Ä–∞–º–∏...\")\n",
    "    query_counts = df['query'].value_counts()\n",
    "    df = df[df['query'].isin(query_counts[query_counts >= 2].index)]\n",
    "\n",
    "    print(\"‚úÇÔ∏è –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ train/val...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        df, test_size=0.1, random_state=42\n",
    "        # , stratify=df['query']\n",
    "    )\n",
    "    print(f\"‚úÖ Train size: {len(train_df)} / Val size: {len(val_df)}\")\n",
    "\n",
    "    print(f\"üìö –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    print(\"üìê –°–æ–∑–¥–∞—ë–º TripletDataset'—ã...\")\n",
    "    train_dataset = TripletDataset(train_df, tokenizer)\n",
    "    val_dataset = TripletDataset(val_df, tokenizer)\n",
    "\n",
    "    print(f\"üìä Train triplets: {len(train_dataset)} / Val triplets: {len(val_dataset)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    print(f\"Train batches: {len(train_loader)} / Val batches: {len(val_loader)} | {batch_size=}\")\n",
    "\n",
    "\n",
    "    return train_loader, val_loader, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîΩ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç tasksource/esci...\n",
      "üì¶ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ pandas DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚Üí –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20278/20278 [00:03<00:00, 6354.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ –§–∏–ª—å—Ç—Ä—É–µ–º –∫–ª–∞—Å—Å—ã: Exact / Substitute / Irrelevant...\n",
      "üîç –£–¥–∞–ª—è–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å < 2 –ø—Ä–∏–º–µ—Ä–∞–º–∏...\n",
      "‚úÇÔ∏è –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ train/val...\n",
      "‚úÖ Train size: 3657 / Val size: 407\n",
      "üìö –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: intfloat/e5-small\n",
      "üìê –°–æ–∑–¥–∞—ë–º TripletDataset'—ã...\n",
      "üìä Train triplets: 36570 / Val triplets: 4070\n",
      "Train batches: 285 / Val batches: 31 | batch_size=128\n"
     ]
    }
   ],
   "source": [
    "# ‚Äî‚Äî‚Äî 3. –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ ‚Äî‚Äî‚Äî\n",
    "train_loader, val_loader, df = prepare_data(sample_rate=0.01, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# https://huggingface.co/intfloat/multilingual-e5-small\n",
    "# --- –ö–ª–∞—Å—Å –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–∞—Ç—á–µ–π ---\n",
    "class E5InferenceModel:\n",
    "    def __init__(self, model_name='intfloat/e5-small', device=None):\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def encode_batch(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids.to(self.device), attention_mask=attention_mask.to(self.device))\n",
    "            return outputs.last_hidden_state[:, 0].cpu().numpy()\n",
    "\n",
    "# --- –ö–ª–∞—Å—Å –¥–ª—è –º–µ—Ç—Ä–∏–∫ –≤–Ω—É—Ç—Ä–∏ –±–∞—Ç—á–∞ ---\n",
    "class RetrievalMetrics:\n",
    "    @staticmethod\n",
    "    def recall_at_k_batch(anchor_embs, product_embs, k_list=[5, 10, 30]):\n",
    "        recalls = {k: 0 for k in k_list}\n",
    "        n = len(anchor_embs)\n",
    "        for i, a_emb in enumerate(anchor_embs):\n",
    "            scores = np.dot(product_embs, a_emb)\n",
    "            top_indices = np.argsort(scores)[-max(k_list):][::-1]\n",
    "            for k in k_list:\n",
    "                # Positive –≤—Å–µ–≥–¥–∞ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ i (–ø–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é TripletDataset)\n",
    "                if i in top_indices[:k]:\n",
    "                    recalls[k] += 1\n",
    "        for k in k_list:\n",
    "            recalls[k] /= n\n",
    "        return recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Pretrain E5 –Ω–∞ –±–∞—Ç—á–∞—Ö: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:04<00:00,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 0.2686\n",
      "Recall@5: 0.5129\n",
      "Recall@10: 0.5862\n",
      "Recall@30: 0.6971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å val_loader ---\n",
    "model_name = 'intfloat/e5-small'\n",
    "device = 'cuda:6' if torch.cuda.is_available() else 'cpu'\n",
    "inference_model = E5InferenceModel(model_name=model_name, device=device)\n",
    "\n",
    "all_recalls = {k: [] for k in [1, 5, 10, 30]}\n",
    "k_list = [1, 5, 10, 30]\n",
    "\n",
    "for batch in tqdm(val_loader, desc=\"üîç Pretrain E5 –Ω–∞ –±–∞—Ç—á–∞—Ö\"):\n",
    "\n",
    "    (anchor_ids, anchor_mask), (pos_ids, pos_mask), (neg_ids, neg_mask), q, pos, neg = batch\n",
    "\n",
    "    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–æ–¥—É–∫—Ç—ã –±–∞—Ç—á–∞ (positive + negative)\n",
    "    batch_product_ids = torch.cat([pos_ids, neg_ids], dim=0)\n",
    "    batch_product_mask = torch.cat([pos_mask, neg_mask], dim=0)\n",
    "\n",
    "    # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "    anchor_embs = inference_model.encode_batch(anchor_ids, anchor_mask)\n",
    "    product_embs = inference_model.encode_batch(batch_product_ids, batch_product_mask)\n",
    "\n",
    "    # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "    recalls = RetrievalMetrics.recall_at_k_batch(anchor_embs, product_embs, k_list=k_list)\n",
    "    for k in k_list:\n",
    "        all_recalls[k].append(recalls[k])\n",
    "\n",
    "# –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º –±–∞—Ç—á–∞–º\n",
    "for k in k_list:\n",
    "    mean_recall = np.mean(all_recalls[k])\n",
    "    print(f\"Recall@{k}: {mean_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: saved_e5_model\n"
     ]
    }
   ],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\n",
    "# –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "SAVE_DIR = \"saved_e5_model\"\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "inference_model.model.save_pretrained(SAVE_DIR)\n",
    "inference_model.tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(f\"‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "_model = AutoModel.from_pretrained(\"saved_e5_model\")\n",
    "_tokenizer = AutoTokenizer.from_pretrained(\"saved_e5_model\")\n",
    "\n",
    "# _model.eval()  # –í–∞–∂–Ω–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "text = \"query: wireless mouse\"\n",
    "inputs = _tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    output = _model(**inputs)\n",
    "    embedding = output.last_hidden_state[:, 0]  # [CLS] —Ç–æ–∫–µ–Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫—É —Å BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "all_products = df['product_title'].dropna().unique().tolist()\n",
    "all_products = [f\"passage: {p}\" for p in all_products]\n",
    "tokenized_products_all = [tokenize(p) for p in all_products]\n",
    "bm25_full = BM25Okapi(tokenized_products_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç BM25 –Ω–∞ –±–∞—Ç—á–∞—Ö: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:12<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1 (BM25): 0.5731\n",
      "Recall@5 (BM25): 0.6822\n",
      "Recall@10 (BM25): 0.7145\n",
      "Recall@30 (BM25): 0.7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "bm25_recalls = defaultdict(list)\n",
    "k_list = [1, 5, 10, 30]\n",
    "\n",
    "product_idx_map = {title: i for i, title in enumerate(all_products)}\n",
    "\n",
    "for batch in tqdm(val_loader, desc=\"üîç BM25 –Ω–∞ –±–∞—Ç—á–∞—Ö\"):\n",
    "    _, _, _, queries, pos_titles, neg_titles = batch\n",
    "\n",
    "    # --- 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º —Å–ø–∏—Å–æ–∫ –∏–∑ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞ ---\n",
    "    batch_products = pos_titles + neg_titles\n",
    "\n",
    "    # --- 2. –ò–Ω–¥–µ–∫—Å—ã —ç—Ç–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ all_products (–¥–ª—è score —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏) ---\n",
    "    batch_indices = [product_idx_map[p] for p in batch_products if p in product_idx_map]\n",
    "\n",
    "    for q, true_title in zip(queries, pos_titles):\n",
    "        q_tokens = tokenize(q)\n",
    "        scores_all = bm25_full.get_scores(q_tokens)\n",
    "\n",
    "        # --- 3. –û—Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ —Å–∫–æ—Ä—ã —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞ ---\n",
    "        scores_batch = [(i, scores_all[i]) for i in batch_indices]\n",
    "        top_indices = sorted(scores_batch, key=lambda x: x[1], reverse=True)\n",
    "        top_titles = [all_products[i] for i, _ in top_indices]\n",
    "        \n",
    "        for k in k_list:\n",
    "            bm25_recalls[k].append(int(true_title in top_titles[:k]))\n",
    "\n",
    "# --- –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ ---\n",
    "for k in k_list:\n",
    "    print(f\"Recall@{k} (BM25): {np.mean(bm25_recalls[k]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–æ–æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ç—Ä–∞–∏–Ω E5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –≤ —Ñ–æ–Ω–µ (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å tmux) –±–æ—Ä–¥—É\n",
    "# # tmux new -s e5_train\n",
    "# tensorboard --logdir tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E5Model(torch.nn.Module):\n",
    "    def __init__(self, model_name='intfloat/e5-small'):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state[:, 0]\n",
    "\n",
    "    def forward(self, anchor_ids, anchor_mask, pos_ids, pos_mask, neg_ids, neg_mask):\n",
    "        anchor_emb = self.encode(anchor_ids, anchor_mask)\n",
    "        pos_emb = self.encode(pos_ids, pos_mask)\n",
    "        neg_emb = self.encode(neg_ids, neg_mask)\n",
    "        return anchor_emb, pos_emb, neg_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_recalls = {k: [] for k in [1, 5, 10, 30]}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # for batch in tqdm(val_loader, desc=\"üîé Eval\"):\n",
    "        for batch in val_loader:\n",
    "            (a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask), _, _, _ = batch\n",
    "\n",
    "            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "            a_ids, a_mask = a_ids.to(device), a_mask.to(device)\n",
    "            p_ids, p_mask = p_ids.to(device), p_mask.to(device)\n",
    "            n_ids, n_mask = n_ids.to(device), n_mask.to(device)\n",
    "\n",
    "            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "            anchor_embs = model.encode(a_ids, a_mask).cpu().numpy()\n",
    "            pos_embs = model.encode(p_ids, p_mask).cpu().numpy()\n",
    "            neg_embs = model.encode(n_ids, n_mask).cpu().numpy()\n",
    "\n",
    "            # –°–æ–±–∏—Ä–∞–µ–º \"–ø—É–ª\" –ø—Ä–æ–¥—É–∫—Ç–æ–≤ (positive + negative)\n",
    "            product_embs = np.concatenate([pos_embs, neg_embs], axis=0)\n",
    "\n",
    "            recalls = RetrievalMetrics.recall_at_k_batch(anchor_embs, product_embs, k_list=k_list)\n",
    "            for k in recalls:\n",
    "                all_recalls[k].append(recalls[k])\n",
    "    \n",
    "    # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –∏ –ø–µ—á–∞—Ç—å\n",
    "    all_means_recalls = {}\n",
    "    for k in all_recalls:\n",
    "        all_means_recalls[k] = np.mean(all_recalls[k])\n",
    "    return all_means_recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CHECKPOINTS_DIR = \"checkpoints\"\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=3, lr=2e-5, device='cuda', every_n_step_do_val=50):\n",
    "    time_suffix = str(datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "    run_name = f\"e5_train_{time_suffix}\" # and model_name!\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = TripletMarginLoss(margin=0.2)\n",
    "    \n",
    "    global_step = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for batch_id, batch in tqdm(enumerate(train_loader), desc=f\"üõ†Ô∏è Epoch {epoch + 1}/{num_epochs}\", total=len(train_loader)):\n",
    "            writer.add_scalar(\"train/epoch_marker\", epoch, global_step)\n",
    "            (a_ids, a_mask), (p_ids, p_mask), (n_ids, n_mask), _, _, _ = batch\n",
    "\n",
    "            a_ids, a_mask = a_ids.to(device), a_mask.to(device)\n",
    "            p_ids, p_mask = p_ids.to(device), p_mask.to(device)\n",
    "            n_ids, n_mask = n_ids.to(device), n_mask.to(device)\n",
    "\n",
    "            anchor, pos, neg = model(a_ids, a_mask, p_ids, p_mask, n_ids, n_mask)\n",
    "            loss = loss_fn(anchor, pos, neg)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1e9)\n",
    "\n",
    "            writer.add_scalar(\"train/loss\", loss.item(), global_step)\n",
    "            writer.add_scalar(\"GradNorm/train\", grad_norm, global_step)\n",
    "            writer.add_scalar(\"LR/train\", optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "            train_recalls = eval_model(model, [batch], device=device)\n",
    "            for k, val in train_recalls.items():\n",
    "                writer.add_scalar(f\"train/recall@{k}\", val, global_step)\n",
    "\n",
    "\n",
    "\n",
    "            if batch_id % every_n_step_do_val == 0:\n",
    "                recalls = eval_model(model, val_loader, device=device)\n",
    "                for k, val in recalls.items():\n",
    "                    writer.add_scalar(f\"val/recall@{k}\", val, global_step)\n",
    "            \n",
    "            global_step += 1    \n",
    "    \n",
    "    print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...\")\n",
    "    save_path = os.path.join(CHECKPOINTS_DIR, f\"{run_name}.pt\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"üì¶ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 285/285 [04:22<00:00,  1.09it/s]\n",
      "üõ†Ô∏è Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 285/285 [04:22<00:00,  1.08it/s]\n",
      "üõ†Ô∏è Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 285/285 [04:22<00:00,  1.09it/s]\n",
      "üõ†Ô∏è Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 285/285 [04:22<00:00,  1.08it/s]\n",
      "üõ†Ô∏è Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 285/285 [04:22<00:00,  1.09it/s]\n",
      "üõ†Ô∏è Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 285/285 [04:22<00:00,  1.08it/s]\n",
      "üõ†Ô∏è Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 285/285 [04:23<00:00,  1.08it/s]\n",
      "üõ†Ô∏è Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 285/285 [04:23<00:00,  1.08it/s]\n",
      "üõ†Ô∏è Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 285/285 [04:24<00:00,  1.08it/s]\n",
      "üõ†Ô∏è Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 285/285 [04:23<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...\n",
      "üì¶ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: checkpoints/e5_train_20250703_175235.pt\n"
     ]
    }
   ],
   "source": [
    "# train_loader, val_loader, _ = prepare_data(batch_size=16, sample_rate=0.05)\n",
    "model = E5Model(model_name='intfloat/e5-small')\n",
    "train_model(model, train_loader, val_loader, num_epochs=3, device='cuda:6', lr=1e-4, every_n_step_do_val=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'query: tshirt jordan men'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[20][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
